{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tdyy343_5UPe",
    "outputId": "a8d1fb8d-1d4e-4d3f-9260-ce12c6ef3fc3"
   },
   "outputs": [],
   "source": [
    "!pip install faiss-cpu\n",
    "\n",
    "# Install Ollama Python SDK\n",
    "!pip install ollama\n",
    "\n",
    "# Install Numpy\n",
    "!pip install numpy\n",
    "\n",
    "# (Optional) Install Transformers if you need it for other models\n",
    "!pip install sentence_transformers\n",
    "\n",
    "# (Optional) Install Pandas if you need it for data handling\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zY6iCGvn5BPM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMU_Traditions.txt\n",
      "CMU_Wikipedia.txt\n",
      "Transportation_Pitts_Wikipedia.txt\n",
      "Pittsburgh_Wikipedia.txt\n",
      "festivals.txt\n",
      "Visit_Pittsburgh.txt\n",
      "current_affairs.txt\n",
      "City_Of_Pittsbugh.txt\n",
      "LocalService_Regulations.txt\n",
      "opera_events.txt\n",
      "ISP_Regulations.txt\n",
      "PIRATES.txt\n",
      "pgh_cultural_trust.txt\n",
      "PENGUINS.txt\n",
      "CMU_History.txt\n",
      "UF_Regulations.txt\n",
      "carnegie_museums.txt\n",
      "Bridges_Wikipedia.txt\n",
      "STEELERS.txt\n",
      "Events_Pittsburgh.txt\n",
      "sports_teams.txt\n",
      "wikis.txt\n",
      "Events_CMU.txt\n",
      "annual_fests.txt\n",
      "CMU_Events_Extra.txt\n",
      "sports_schedules.txt\n",
      "History_WIkipedia.txt\n",
      "museums_list.txt\n",
      "CMU.txt\n",
      "Visit_Pitts 1.txt\n",
      "symphony.txt\n",
      "Culture_Pitts_wikipedia.txt\n",
      "Parking_Regulations.txt\n",
      "Amusement_Regulations.txt\n",
      "CMU_Welcome_To_Pittsburgh.txt\n",
      "Payroll_Regulations.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory containing text files\n",
    "directory = \"../data/\"\n",
    "\n",
    "# Read all text files and combine them\n",
    "all_texts = []\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        print(filename)\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "            all_texts.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HkFD6uTt5MOm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2909\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, max_length=128):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(words), max_length):\n",
    "        chunks.append(\" \".join(words[i:i + max_length]))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Create chunks from each file content\n",
    "text_chunks = []\n",
    "for text in all_texts:\n",
    "    text_chunks.extend(chunk_text(text, max_length=256))\n",
    "print(len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_DwTBSf96DU1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained model for embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings = model.encode(text_chunks, convert_to_tensor=True)\n",
    "embedding_dim = doc_embeddings.shape[1]\n",
    "\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(doc_embeddings.cpu().numpy())\n",
    "\n",
    "faiss.write_index(index, 'faiss_index.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lTJ-7Smp6527"
   },
   "outputs": [],
   "source": [
    "def retrieve_documents(query, k=3):\n",
    "    query_embedding = model.encode([query])\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    matching_chunks = [text_chunks[i] for i in indices[0]]\n",
    "    return matching_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OtLxcvbt6758"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "#If there are ways to frame the answer in multiple ways, return multiple answers separated by semicolons.\n",
    "\n",
    "def ollama_generate(question, context):\n",
    "    instructions = f'''\n",
    "    Answer the question based on the provided context. \n",
    "    Give short answers.\n",
    "    For example:\n",
    "    Q1: What are the operating hours of the CMU Pantry?\n",
    "    A1: 2 p.m. to 5 p.m.\n",
    "    While answering a question about some event X, do not use \"it\" or \"the event\", mention the name of event X in the answer.\n",
    "    For example:\n",
    "    Q3: How long does the Gender in Process event last?\n",
    "    Wrong answer: This event runs from 3:30 to 5 p.m.\n",
    "    Correct answer: 3:30 to 5 p.m.\n",
    "    Don't give dates like \"11/21\", instead the answer should be \"November 21, 2024\"\n",
    "    '''\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"llama2\",\n",
    "        \"prompt\": f\"Question: {question}\\nContext: {context}\\nInstructions: {instructions}\\nAnswer:\",\n",
    "        \"stream\": False,\n",
    "        # \"temperature\": 0.5,  # Reduce to make output more deterministic\n",
    "        # \"top_p\": 0.9,        # Control randomness\n",
    "        \"max_tokens\": 150    # Limit response length\n",
    "    }\n",
    "    response = requests.post(url, json=payload)\n",
    "    return response.json()['response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 686
    },
    "id": "eB-kSzOo7GNF",
    "outputId": "5fa6e812-6956-4cbb-fb4b-662499c5fb46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the colours of Pittsburgh Steelers?\n",
      "Answer: Gold and black.\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the colours of Pittsburgh Steelers?\"\n",
    "retrieved_chunks = retrieve_documents(question)\n",
    "# print(retrieved_chunks)\n",
    "\n",
    "context = \" \".join(retrieved_chunks)\n",
    "answer = ollama_generate(question, context)\n",
    "print(f\"Question: {question}\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/akshitagupta/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import requests\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "df = pd.read_csv('../final-dataset/test.csv')\n",
    "\n",
    "generated_answers = []\n",
    "\n",
    "# Iterate through each row in the DataFrame and get answers\n",
    "for _, row in df.iterrows():\n",
    "    question = row['Question']\n",
    "    retrieved_chunks = retrieve_documents(question)\n",
    "    context = \" \".join(retrieved_chunks)\n",
    "    answer = ollama_generate(question, context)\n",
    "    # answer = ollama_generate(question)\n",
    "    generated_answers.append(answer)\n",
    "\n",
    "# Add the generated answers to the DataFrame\n",
    "df['Generated Answer'] = generated_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "def calculate_metrics(predicted, actual):\n",
    "    predicted = predicted.strip().lower()\n",
    "    actual_answers = [ans.strip().lower() for ans in actual.split(';')]\n",
    "    \n",
    "    predicted = predicted.translate(str.maketrans('', '', string.punctuation))\n",
    "    actual_answers = [ans.translate(str.maketrans('', '', string.punctuation)) for ans in actual_answers]\n",
    "    \n",
    "    predicted_tokens = word_tokenize(predicted)\n",
    "    actual_tokens = [word_tokenize(ans) for ans in actual_answers]\n",
    "    \n",
    "    exact_match = any(predicted == ans for ans in actual_answers)\n",
    "    \n",
    "    f1_scores = []\n",
    "    for ans in actual_tokens:\n",
    "        true_positives = len(set(predicted_tokens) & set(ans))\n",
    "        precision = true_positives / len(predicted_tokens) if predicted_tokens else 0\n",
    "        recall = true_positives / len(ans) if ans else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    max_f1 = max(f1_scores)\n",
    "    \n",
    "    answer_recall = any(all(word in predicted_tokens for word in ans) for ans in actual_tokens)\n",
    "    \n",
    "    return exact_match, max_f1, answer_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Exact Match: 0.0345\n",
      "Average F1 Score: 0.2297\n",
      "Average Answer Recall: 0.2759\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for _, row in df.iterrows():\n",
    "    predicted = row['Generated Answer']\n",
    "    actual = row['Answer']\n",
    "    \n",
    "    exact_match, f1_score, answer_recall = calculate_metrics(predicted, actual)\n",
    "    \n",
    "    results.append({\n",
    "        'question': row['Question'],\n",
    "        'predicted_answer': predicted,\n",
    "        'actual_answer': actual,\n",
    "        'exact_match': exact_match,\n",
    "        'f1_score': f1_score,\n",
    "        'answer_recall': answer_recall\n",
    "    })\n",
    "\n",
    "# Add the results to the DataFrame\n",
    "df['Exact Match'] = [r['exact_match'] for r in results]\n",
    "df['F1 Score'] = [r['f1_score'] for r in results]\n",
    "df['Answer Recall'] = [r['answer_recall'] for r in results]\n",
    "\n",
    "# Calculate averages\n",
    "average_exact_match = np.mean(df['Exact Match'])\n",
    "average_f1 = np.mean(df['F1 Score'])\n",
    "average_answer_recall = np.mean(df['Answer Recall'])\n",
    "\n",
    "# Print overall average metrics\n",
    "print(f\"Average Exact Match: {average_exact_match:.4f}\")\n",
    "print(f\"Average F1 Score: {average_f1:.4f}\")\n",
    "print(f\"Average Answer Recall: {average_answer_recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
