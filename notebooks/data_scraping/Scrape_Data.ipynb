{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4 requests pdfkit pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNQinpCgyaNh",
        "outputId": "7a53517f-8c19-428f-c756-1fb580ca5ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Collecting pdfkit\n",
            "  Downloading pdfkit-1.0.0-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Downloading pdfkit-1.0.0-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: pdfkit\n",
            "Successfully installed pdfkit-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MAIN CODE WIKIPEDIA"
      ],
      "metadata": {
        "id": "Z4YYjy0S4Oht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "\n",
        "def scrape_wikipedia_page(url):\n",
        "    try:\n",
        "\n",
        "        response = requests.get(url, verify=False)\n",
        "        response.raise_for_status()\n",
        "\n",
        "\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "\n",
        "        title_tag = soup.find('h1')\n",
        "        if title_tag:\n",
        "            title = title_tag.get_text(strip=True)\n",
        "        else:\n",
        "            title = \"Title not found\"\n",
        "\n",
        "\n",
        "        sections = {}\n",
        "        current_section = title\n",
        "        sections[current_section] = []\n",
        "\n",
        "\n",
        "        for tag in soup.find_all(['h2', 'h3', 'h4', 'p', 'ul', 'ol', 'table']):\n",
        "            if tag.name in ['h2', 'h3', 'h4']:\n",
        "                current_section = tag.get_text(strip=True)\n",
        "                sections[current_section] = []\n",
        "\n",
        "\n",
        "\n",
        "            elif tag.name == 'p':\n",
        "                paragraph_text = ''\n",
        "\n",
        "                for content in tag.contents:\n",
        "                    if isinstance(content, str):\n",
        "                      paragraph_text += content.strip()\n",
        "                    elif content.name == 'a':\n",
        "                      paragraph_text += ' ' + content.get_text(strip=True) + ' '\n",
        "\n",
        "                # Normalize spaces and add the paragraph text to the section\n",
        "                sections[current_section].append(re.sub(r'\\s+', ' ', paragraph_text).strip())\n",
        "\n",
        "\n",
        "            elif tag.name in ['ul', 'ol']:  # Lists (unordered and ordered)\n",
        "                list_items = [li.get_text(strip=True) for li in tag.find_all('li')]\n",
        "                sections[current_section].append(\"\\n\".join(f\"â€¢ {item}\" for item in list_items))\n",
        "            elif tag.name == 'table':  # Tables\n",
        "                table_data = []\n",
        "                rows = tag.find_all('tr')\n",
        "                for row in rows:\n",
        "                    cells = row.find_all(['th', 'td'])\n",
        "                    row_text = [cell.get_text(strip=True) for cell in cells]\n",
        "                    table_data.append(\" | \".join(row_text))  # Join cells with a pipe separator for readability\n",
        "                sections[current_section].append(\"\\n\".join(table_data))\n",
        "\n",
        "        return title, sections\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Function to save scraped data to a text file\n",
        "def save_scraped_data_to_file(title, sections, output_file):\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        # Save the title of the page at the top\n",
        "        file.write(f\"Title: {title}\\n\")\n",
        "        file.write(\"=\" * len(title) + \"\\n\\n\")\n",
        "\n",
        "        # Save each section with its heading and content (including lists and tables)\n",
        "        for heading, content in sections.items():\n",
        "            file.write(f\"{heading}\\n\")\n",
        "            file.write(\"-\" * len(heading) + \"\\n\")  # Underline the heading\n",
        "            for item in content:\n",
        "                file.write(f\"{item}\\n\\n\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    wiki_url = \"https://pittsburghpa.gov/mayor/index.html\"\n",
        "\n",
        "\n",
        "    output_file = \"CityHall.txt\"\n",
        "\n",
        "\n",
        "    title, sections = scrape_wikipedia_page(wiki_url)\n",
        "\n",
        "\n",
        "    if title and sections:\n",
        "\n",
        "        save_scraped_data_to_file(title, sections, output_file)\n",
        "        print(f\"Scraped data saved to {output_file}\")\n",
        "    else:\n",
        "        print(\"Failed to scrape the Wikipedia page.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Jejam1evrub",
        "outputId": "094b64c6-3262-44d5-ac9d-a0d62b474d28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py:1099: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pittsburghpa.gov'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraped data saved to CityHall.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrape Links in Text File"
      ],
      "metadata": {
        "id": "Ukpl37hX32H4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_url(url):\n",
        "\n",
        "    return url.startswith(\"http://\") or url.startswith(\"https://\")\n",
        "\n",
        "\n",
        "def scrape_content(link):\n",
        "    cleaned_link = link.strip().rstrip(\"\\\\/\")\n",
        "\n",
        "    if not is_valid_url(cleaned_link):\n",
        "        print(f\"Skipping invalid URL: {cleaned_link}\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        response = requests.get(cleaned_link)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        headings = [heading.get_text().strip() for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])]\n",
        "        paragraphs = [para.get_text().strip() for para in soup.find_all('p')]\n",
        "\n",
        "        with open(\"scraped_content.txt\", \"a\", encoding=\"utf8\") as out_file:\n",
        "            out_file.write(f\"URL: {cleaned_link}\\n\")\n",
        "            out_file.write(\"Headings:\\n\")\n",
        "            for heading in headings:\n",
        "                out_file.write(f\"{heading}\\n\")\n",
        "            out_file.write(\"\\nParagraphs:\\n\")\n",
        "            for para in paragraphs:\n",
        "                out_file.write(f\"{para}\\n\")\n",
        "            out_file.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error scraping {cleaned_link}: {e}\")\n",
        "\n",
        "\n",
        "with open(\"/content/Links.rtf\", \"r\") as file:\n",
        "    for link in file.readlines():\n",
        "        scrape_content(link)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-ydUrts31pN",
        "outputId": "028c2436-5fac-45cb-f0b2-453afab9ad7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping invalid URL: {\\rtf1\\ansi\\ansicpg1252\\cocoartf2761\n",
            "Skipping invalid URL: \\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fswiss\\fcharset0 Helvetica;}\n",
            "Skipping invalid URL: {\\colortbl;\\red255\\green255\\blue255;}\n",
            "Skipping invalid URL: {\\*\\expandedcolortbl;;}\n",
            "Skipping invalid URL: \\margl1440\\margr1440\\vieww11520\\viewh8400\\viewkind0\n",
            "Skipping invalid URL: \\pard\\tx720\\tx1440\\tx2160\\tx2880\\tx3600\\tx4320\\tx5040\\tx5760\\tx6480\\tx7200\\tx7920\\tx8640\\pardirnatural\\partightenfactor0\n",
            "Skipping invalid URL: \n",
            "Skipping invalid URL: \\f0\\fs24 \\cf0 https://www.visitpittsburgh.com/blog/pittsburgh-vegan-restaurants\n",
            "Error scraping https://www.sportspittsburgh.com/about-sportspittsburgh/}: 404 Client Error: Not Found for url: https://www.sportspittsburgh.com/about-sportspittsburgh/%7D/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "import re\n",
        "\n",
        "from urllib3.exceptions import InsecureRequestWarning\n",
        "warnings.simplefilter('ignore', InsecureRequestWarning)\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "    text = re.sub(r'[^\\w\\s.,\\'\"!?]', ' ', text)\n",
        "\n",
        "    text = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', text)\n",
        "\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def scrape_page(url, content_file):\n",
        "    try:\n",
        "        response = requests.get(url, verify=False)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "\n",
        "            title = soup.find('title').get_text(\" \", strip=True) if soup.find('title') else 'No Title'\n",
        "            content_file.write(f\"Page URL: {url}\\n\")\n",
        "            content_file.write(f\"Title: {clean_text(title)}\\n\\n\")\n",
        "\n",
        "\n",
        "            paragraphs = soup.find_all('p')\n",
        "            for paragraph in paragraphs:\n",
        "                cleaned_paragraph = clean_text(paragraph.get_text(\" \", strip=True))\n",
        "                content_file.write(cleaned_paragraph + '\\n')\n",
        "\n",
        "\n",
        "            lists = soup.find_all(['ul', 'ol'])\n",
        "            for lst in lists:\n",
        "                list_items = lst.find_all('li')\n",
        "                for item in list_items:\n",
        "                    cleaned_item = clean_text(item.get_text(\" \", strip=True))\n",
        "                    content_file.write(f\"- {cleaned_item}\\n\")\n",
        "\n",
        "\n",
        "            tables = soup.find_all('table')\n",
        "            for table in tables:\n",
        "                rows = table.find_all('tr')\n",
        "                for row in rows:\n",
        "                    columns = row.find_all(['th', 'td'])\n",
        "                    cleaned_row = ' | '.join([clean_text(col.get_text(\" \", strip=True)) for col in columns])\n",
        "                    content_file.write(cleaned_row + '\\n')\n",
        "\n",
        "            content_file.write(\"\\n--- End of page content ---\\n\\n\")\n",
        "        else:\n",
        "            print(f\"Failed to scrape {url}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {e}\")\n",
        "\n",
        "\n",
        "def find_subpages(main_url):\n",
        "    try:\n",
        "        response = requests.get(main_url, verify=False)\n",
        "        subpage_links = []\n",
        "\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "\n",
        "            for link in soup.find_all('a', href=True):\n",
        "                href = link['href']\n",
        "\n",
        "                full_url = urljoin(main_url, href)\n",
        "\n",
        "\n",
        "                if main_url in full_url:\n",
        "                    subpage_links.append(full_url)\n",
        "\n",
        "        return subpage_links\n",
        "    except Exception as e:\n",
        "        print(f\"Error finding subpages: {e}\")\n",
        "        return []\n",
        "\n",
        "def scrape_website_with_subpages(main_url, content_file, subpages_file):\n",
        "    with open(content_file, 'w', encoding='utf-8') as content, open(subpages_file, 'w', encoding='utf-8') as subpages:\n",
        "        scrape_page(main_url, content)\n",
        "        subpage_links = find_subpages(main_url)\n",
        "\n",
        "        print(f\"Found {len(subpage_links)} subpages to scrape.\\n\")\n",
        "        content.write(f\"Found {len(subpage_links)} subpages to scrape.\\n\\n\")\n",
        "        subpages.write(f\"Subpages scraped from {main_url}:\\n\\n\")\n",
        "\n",
        "        for subpage in subpage_links:\n",
        "            subpages.write(subpage + '\\n')\n",
        "            scrape_page(subpage, content)\n",
        "\n",
        "\n",
        "main_url = \"https://pittsburghpa.gov/mayor/city-staff\"\n",
        "content_file = \"scraped_content.txt\"\n",
        "subpages_file = \"scraped_subpages.txt\"\n",
        "scrape_website_with_subpages(main_url, content_file, subpages_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V92AU4dgCXX",
        "outputId": "d0bfe00d-c716-4c4e-c8e6-cd84e9794aa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 12 subpages to scrape.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}