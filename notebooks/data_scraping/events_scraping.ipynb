{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5568d5ec-07dd-4ee8-a7cf-faca683c9897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.25.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.2)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.26.2-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (2024.7.4)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /opt/anaconda3/lib/python3.12/site-packages (from selenium) (1.8.0)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium)\n",
      "  Downloading attrs-24.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: sortedcontainers in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/anaconda3/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Downloading selenium-4.25.0-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.26.2-py3-none-any.whl (475 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.0/476.0 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading attrs-24.2.0-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: wsproto, attrs, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "Successfully installed attrs-24.2.0 outcome-1.3.0.post0 selenium-4.25.0 trio-0.26.2 trio-websocket-0.11.1 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "65384b4e-b92d-48f6-9a2f-feb4c44aa263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "104bac32-a4af-48e4-a75f-463508a6067b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "Scraping https://www.mlb.com/pirates/ballpark/pirates-clubhouse-store...\n",
      "Scraping https://www.mlb.com/pirates/fans/kids/...\n",
      "Scraping https://www.mlb.com/athletics...\n",
      "Scraping https://www.mlb.com/pirates/ballpark/history...\n",
      "Scraping https://www.mlb.com/pirates/fans/jim-leyland-hall-of-fame...\n",
      "Scraping https://www.mlb.com/astros...\n",
      "Scraping https://www.mlbshop.com/pittsburgh-pirates/big-and-tall/t-14892280+es-35+z-910315-2495592026?_s=bm-mlbcom-pit...\n",
      "Scraping https://www.mlb.com/dbacks...\n",
      "Scraping https://www.mlb.com/pirates/video...\n",
      "Scraping https://www.mlb.com/pirates/roster...\n",
      "Scraping https://www.mlb.com/pirates/ballpark/information/gameday...\n",
      "Scraping https://www.mlb.com/pirates/video/topic/t134-default-vtp...\n",
      "Scraping https://www.mlb.com/pirates/tickets/group-tickets...\n",
      "Scraping https://thepiratespress.mlblogs.com/...\n",
      "Scraping https://www.mlb.com/pirates/ballpark/tours...\n",
      "Scraping https://www.mlb.com/pirates/news/top-prospects-going-to-afl-2024?t=arizona-fall-league-coverage...\n",
      "Scraping https://www.mlb.com/marlins...\n",
      "Scraping https://www.mlbshop.com/gift-cards/x-462351+z-94899005-3509039474?_s=bm-mlbcom-pit...\n",
      "Scraping https://www.mlb.com/pirates/history...\n",
      "Scraping https://seatgeek.com/pittsburgh-pirates-tickets?aid=15985&pid=integration&rid=1&utm_medium=partnership&utm_source=pirates_sponsorship&utm_campaign=integration...\n",
      "Scraping https://www.mlbshop.com/pittsburgh-pirates/home-and-office/t-14674480+d-6738552333+z-97-3817295593?_s=bm-mlbcom-pit...\n",
      "Scraping https://www.mlb.com/pirates/tickets/season-tickets...\n",
      "Scraping https://www.mlb.com/pirates/video/run-it-back-with-liover-peguero?t=run-it-back...\n",
      "Scraping https://www.mlb.com/official-information/accessibility...\n",
      "Scraping https://www.mlb.com/pirates/fans/experiences-merchandise...\n",
      "Scraping https://www.mlb.com/pirates/sponsorship...\n",
      "Scraping https://www.mlb.com/pirates/ballpark/seating-map/3d...\n",
      "Scraping https://www.mlb.com/pirates/team/front-office...\n",
      "Scraping https://www.mlb.com/pirates/fans/souvenir-tickets...\n",
      "Scraping https://www.mlb.com/whitesox...\n",
      "Scraping https://www.mlb.com/mariners...\n",
      "Scraping https://www.mlb.com/pirates/tickets/information-request...\n",
      "Scraping https://www.mlb.com/pirates/ballpark/enhancements...\n",
      "Scraping https://www.mlb.com/rockies...\n",
      "Scraping https://www.mlb.com/rangers...\n",
      "Scraping https://www.mlb.com/pirates/stats/team...\n",
      "Scraping https://www.mlb.com/pirates/video/topic/pirates-game-recap...\n",
      "Scraping https://www.mlbshop.com/pittsburgh-pirates/collectibles-and-memorabilia/t-14784435+d-6783338849+z-93-4025914707?_s=bm-mlbcom-pit...\n",
      "Scraping http://www.mlbshopeurope.com/stores/mlb/en/c/shop-by-team/national-league/pittsburgh-pirates?portal=AEI46DFD&CMP=PSC-AEI46DFD...\n",
      "Scraping https://www.mlb.com/redsox...\n",
      "Scraping https://www.mlb.com/pirates/news/arizona-fall-league-opening-day-guide-2024?t=arizona-fall-league-coverage...\n",
      "Scraping https://www.mlb.com/pirates/news/topic/pirates-press-releases...\n",
      "Scraping https://www.bgca.org/about-us/our-partners/mlb...\n",
      "Scraping https://www.mlb.com/cubs...\n",
      "Scraping https://www.mlb.com/pirates/ballpark/features...\n",
      "Scraping https://www.mlbshop.com/pittsburgh-pirates/caps/t-14129980+d-6705223325+z-95-1745244677?_s=bm-mlbcom-pit...\n",
      "Scraping https://www.mlb.com/pirates/...\n",
      "Scraping https://www.mlb.com/live-stream-games/subscribe?affiliateId=clubMENU...\n",
      "Scraping https://www.mlb.com/pirates/fans/kids-club...\n",
      "Scraping https://www.mlb.com/pirates/schedule/printable-schedule...\n",
      "Scraping https://www.mlb.com/news...\n",
      "Scraping https://www.mlb.com/pirates/ballpark/information...\n",
      "Scraping https://www.mlb.com/pirates/schedule/2025-02...\n",
      "Scraping https://www.mlb.com/pirates/fans/mascots...\n",
      "Scraping https://www.mlb.com/pirates/ballpark...\n",
      "Scraping https://pirates.auctions.mlb.com/?_gl=1*153jogc*_gcl_aw*R0NMLjE2ODEzMTA3NTguQ2p3S0NBandyZG1oQmhCQkVpd0E0SHg1Z3o2dl9TZW54SkRtTFl4REpsRVBhTHRvSmlQUjFHZTY4SGpVWmYtU0lrTWdURmdDUU9kMmVSb0NnTk1RQXZEX0J3RQ..*_gcl_dc*R0NMLjE2ODEzMTA3NTguQ2p3S0NBandyZG1oQmhCQkVpd0E0SHg1Z3o2dl9TZW54SkRtTFl4REpsRVBhTHRvSmlQUjFHZTY4SGpVWmYtU0lrTWdURmdDUU9kMmVSb0NnTk1RQXZEX0J3RQ..&utm_source=Nav+Auction+Tray+PIT&utm_medium=Club.com+Shop+Nav&utm_campaign=Club.com+permanent+Navigation...\n",
      "Scraping https://www.mlb.com/pirates/ballpark/events...\n",
      "Scraping https://www.mlb.com/official-information/authentication...\n",
      "Scraping https://www.mlb.com/prospects/2024/pirates/...\n",
      "Scraping https://www.mlb.com/pirates/team/photos...\n",
      "Scraping https://www.mlb.com/pirates/tickets/events...\n",
      "Scraping https://www.mlb.com/reds...\n",
      "Scraping https://www.mlb.com/pirates/tickets...\n",
      "Scraping https://www.mlb.com/twins...\n",
      "Scraping https://www.mlb.com/pirates/tickets/single-game-tickets...\n",
      "Scraping https://www.mlb.com/pirates/fans/piratesfest...\n",
      "Scraping https://www.mlb.com/angels...\n",
      "Scraping https://www.mlb.com/pirates/schedule/2025-03...\n",
      "Scraping https://www.mlb.com/pirates/scores...\n",
      "Scraping https://www.mlb.com/pirates/roster/depth-chart...\n",
      "Scraping https://www.mlb.com/pirates/news...\n",
      "Scraping https://www.mlb.com/pirates/community/green-initiatives...\n",
      "Scraping https://www.mlb.com/prospects/stats/affiliates?teamId=134...\n",
      "Scraping https://www.mlb.com/pirates/official-information...\n",
      "Scraping https://www.mlb.com/pirates/tickets/mobile/faqs...\n",
      "Scraping https://www.mlb.com/yankees...\n",
      "Scraping https://www.mlb.com/pirates/news/topic/pirates-injury-report...\n",
      "Scraping https://www.mlbshop.com/pittsburgh-pirates/kids/t-47012280+ga-92+z-997947-3537571025?_s=bm-mlbcom-pit...\n",
      "Scraping https://www.mlb.com/pirates...\n",
      "Scraping https://www.milb.com/about/teams/by-affiliate#pittsburgh-pirates...\n",
      "Scraping https://www.mlb.com/mets...\n",
      "Scraping https://www.mlb.com/events...\n",
      "Scraping https://www.mlb.com/pirates/team/broadcasters...\n",
      "Scraping https://www.mlbshop.com/trading-cards/d-4507487451+z-952513-2008381280?_s=bm-mlb.com-ShopButton-2024-TradingCards-pit...\n",
      "Scraping https://www.mlb.com/pirates/standings...\n",
      "Scraping https://www.mlb.com/pirates/schedule/2025/fullseason...\n",
      "Scraping https://www.mlbshop.com/pittsburgh-pirates/t-14121179+z-9812019-458107067?os=1&_s=bm-mlbcom-pit...\n",
      "Scraping https://www.mlb.com/pirates/news/topic/pirates-pipeline-coverage...\n",
      "Scraping https://www.mlb.com/padres...\n",
      "Scraping https://www.mlb.com/pirates/social...\n",
      "Scraping https://www.mlb.com/bluejays...\n",
      "Scraping https://www.mlb.com/team...\n",
      "Scraping https://www.mlb.com/all-mlb/ballot?affiliateId=allmlb-club-trending-2024...\n",
      "Scraping https://www.mlb.com/pirates/history/hall-of-fame...\n",
      "Scraping https://www.mlbshop.com/pittsburgh-pirates/jerseys/t-14343380+d-1261223436+z-92-875728927?_s=bm-mlbcom-pit...\n",
      "Scraping https://www.mlb.com/pirates/video/topic/run-it-back...\n",
      "Scraping https://www.mlb.com/pirates/apps...\n",
      "Scraping https://www.mlb.com/phillies...\n",
      "Scraping https://www.mlb.com/pirates/roster/40-man...\n",
      "Scraping https://www.mlb.com/pirates/fans/gift-guide...\n",
      "Scraping https://www.mlb.com/pirates/community/pirates-charities...\n",
      "Scraping https://www.mlb.com/pirates/apps/ballpark...\n",
      "Scraping https://www.mlb.com/...\n",
      "Scraping https://www.mlb.com/pirates/stats...\n",
      "Scraping https://www.mlb.com/pirates/news/ranking-arizona-fall-league-teams-2024?t=arizona-fall-league-coverage...\n",
      "Scraping https://www.mlb.com/es/pirates...\n",
      "Scraping https://www.mlb.com/pirates/roster/transactions...\n",
      "Scraping https://www.mlb.com/brewers...\n",
      "Scraping https://www.mlb.com/pirates/video/run-it-back-with-andrew-mccutchen?t=run-it-back...\n",
      "Scraping https://www.mlb.com/cardinals...\n",
      "Scraping https://www.mlb.com/dodgers...\n",
      "Scraping https://www.mlb.com/pirates/news/pirates-fire-three-coaches-after-2024-season...\n",
      "Scraping https://www.mlb.com/pirates/news/ben-cherington-on-pirates-offense-bullpen-after-coaching-changes...\n",
      "Scraping https://www.mlb.com/draft/tracker/all/team/pirates...\n",
      "Scraping https://baseballsavant.mlb.com/team/134...\n",
      "Scraping https://www.mlb.com/braves...\n",
      "Scraping https://www.mlb.com/players...\n",
      "Scraping https://www.mlb.com/pirates/fans...\n",
      "Scraping https://www.mlb.com/orioles...\n",
      "Scraping https://www.mlb.com/royals...\n",
      "Scraping https://www.mlb.com/pirates/feeds/news/rss.xml...\n",
      "Scraping https://www.mlb.com/pirates/roster/coaches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/html/parser.py:171: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  k = self.parse_starttag(i)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping https://www.mlb.com/prospects/pirates/...\n",
      "Scraping https://www.mlb.com/pirates/fans/small-business-non-profit...\n",
      "Scraping https://www.mlbshop.com/pittsburgh-pirates/women/t-25455502+ga-68+z-994768-2409408569?_s=bm-mlbcom-pit...\n",
      "Scraping https://www.mlb.com/rays...\n",
      "Scraping https://www.mlb.com/pirates/news/pirates-injuries-and-roster-moves...\n",
      "Scraping https://www.mlb.com/guardians...\n",
      "Scraping https://www.mlb.com/pirates/team/jobs...\n",
      "Scraping https://www.mlb.com/pirates/tickets/group-tickets/hospitality-and-suites...\n",
      "Scraping https://www.mlb.com/pirates/news/updated-top-100-prospects-list-for-october-2024?t=mlb-pipeline-coverage...\n",
      "Scraping https://mlb.tickets.com/?agency=PIRM_MYTIXX&orgid=39910#/auth/login...\n",
      "Scraping https://www.mlbshop.com/pittsburgh-pirates/men/t-25671124+ga-89+z-921841-1278411202?_s=bm-mlbcom-pit...\n",
      "Scraping https://www.mlb.com/nationals...\n",
      "Scraping https://www.mlb.com/prospects/stats?teamId=134...\n",
      "Scraping https://www.mlb.com/giants...\n",
      "Scraping https://www.mlb.com/pirates/video/topic/pirates-community...\n",
      "Scraping https://auctions.mlb.com/iSynApp/showHomePage.action?sid=1101001&isynsharedsession=mki8i1dARcxDt3WwReTi6kJOREHftyZJNK-Xg3m10ZpEdaSACl02r4aCDzYhNvUm&utm_source=Nav+Auction+Tray+PIT&utm_medium=Club.com+Shop+Nav+MLB&utm_campaign=Club.com+permanent+Navigation...\n",
      "Scraping https://www.mlb.com/tigers...\n",
      "Scraping https://www.mlb.com/pirates/tickets/premium...\n",
      "Scraping https://www.mlb.com/pirates/news/all-mlb-team-2024-nominees-by-team...\n",
      "Scraping https://support.mlb.com/s/...\n",
      "Scraping https://www.mlb.com/pirates/fans/newsletters...\n",
      "Scraping https://www.mlbshop.com/pittsburgh-pirates/t-14121179+z-9812019-458107067?_s=bm-mlbcom-pit...\n",
      "Scraping https://www.mlb.com/pirates/schedule/downloadable-schedule...\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "# Base URL of the website\n",
    "base_url = \"https://www.mlb.com/pirates\"\n",
    "\n",
    "f = open(\"PIRATES.txt\", \"w+\")\n",
    "\n",
    "# Send a request to the website\n",
    "response = requests.get(base_url, verify=False)\n",
    "print(response)\n",
    "\n",
    "# Parse the content\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all anchor tags\n",
    "anchors = soup.find_all('a')\n",
    "\n",
    "# Extract and format all subpage links\n",
    "subpages = set()\n",
    "for anchor in anchors:\n",
    "    href = anchor.get('href')\n",
    "    if href:\n",
    "        full_url = urljoin(base_url, href)\n",
    "        subpages.add(full_url)\n",
    "\n",
    "def scrape_subpage_content(url):\n",
    "    try:\n",
    "        subpage_response = requests.get(url, verify=False)\n",
    "        subpage_soup = BeautifulSoup(subpage_response.text, \"html.parser\")\n",
    "\n",
    "        # Scrape relevant content (adjust based on site structure)\n",
    "        content = subpage_soup.find_all('p')  # Example: scraping paragraphs\n",
    "        page_content = \"\\n\".join([p.get_text() for p in content])\n",
    "        return page_content\n",
    "    except Exception as e:\n",
    "        return f\"Error scraping {url}: {e}\"\n",
    "\n",
    "# Scrape each subpage\n",
    "for subpage in subpages:\n",
    "    print(f\"Scraping {subpage}...\")\n",
    "    if subpage == \"https://mlb.tickets.com/?agency=PIRM_MYTIXX&orgid=39910#/auth/login\":\n",
    "        continue\n",
    "    try:\n",
    "        content = scrape_subpage_content(subpage)\n",
    "        f.write(content)\n",
    "    except:\n",
    "        print(\"could not scrape \" + subpage)\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "# content = scrape_subpage_content(\"https://www.heinzhistorycenter.org/events/\")\n",
    "# print(content)\n",
    "# print(subpages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "22ee0a63-f2d2-4df6-af36-452d1602d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8a898443-a0dd-4bf8-bef9-61dc4c852c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to scrape\n",
    "\n",
    "import re\n",
    "\n",
    "url = \"https://pittsburgh.events/\"\n",
    "\n",
    "f = open(\"Events.txt\", \"w+\")\n",
    "\n",
    "months = [\"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"]\n",
    "count_events = 0\n",
    "\n",
    "# Request the webpage content\n",
    "def get_data_from_url(url, count_events, file = f):\n",
    "    # driver = webdriver.Chrome()\n",
    "    # driver.get(url)\n",
    "    # Get the page source after interactions\n",
    "    # page_source = driver.page_source\n",
    "    response = requests.get(url, verify = False)\n",
    "    # soup = BeautifulSoup(page_source, 'html.parser')\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    events = soup.find_all('div', class_='date')\n",
    "    count_events+=len(events)\n",
    "    \n",
    "    for event in events:\n",
    "        # title = event.find('h2').get_text()  # Scraping event title\n",
    "        date = event.find('div', class_='date')\n",
    "        month = event.find('div', class_='month').get_text()\n",
    "        day = event.find('div', class_='day').get_text()\n",
    "        year = event.find('div', class_='year').get_text()\n",
    "        full_date = f\"{month} {day}, {year}\"\n",
    "\n",
    "        # location = event.find('div', class_='location').get_text()  # Scraping location\n",
    "        # time = event.find_next_sibling('div', class_='time').get_text() if event.find_next_sibling('div', class_='time') else \"No time provided\"\n",
    "        # print(time)\n",
    "        event_link = event.find_next('div', class_='venue').find('div')\n",
    "        # event_name = event_link.get_text(strip=True) if event_link else \"No event name\"\n",
    "        text_before_a = event_link.contents[0].strip() if event_link.contents else \"\"\n",
    "        \n",
    "        # Get text inside the <a> tag\n",
    "        text_in_a = event_link.find('a').get_text(strip=True) if event_link.find('a') else \"\"\n",
    "        \n",
    "        # Combine with a space\n",
    "        event_name = f\"{text_before_a} {text_in_a}\".strip()\n",
    "        # location = event_link.get_text(strip=True).replace(event_name, '').strip()\n",
    "        location_centre = event_link.find_next('div', class_='date-desc')\n",
    "        location_div = event_link.find_next('span', class_='location')\n",
    "        location = location_centre.get_text(strip=True) + \" \" + location_div.get_text(strip=True)\n",
    "        link = event_link.find('a')['href'] if event_link.find('a') else \"\"\n",
    "    \n",
    "        # Separate the pincode (assuming it's a 5-digit number followed by the address)\n",
    "        # pincode_match = re.search(r'\\b\\d{5}\\b', location)  # Match 5-digit pincode\n",
    "        # if pincode_match:\n",
    "        #     pincode = pincode_match.group(0)  # Get the pincode\n",
    "        #     # Split location at the pincode\n",
    "        #     location_without_pincode = location.split(pincode)[0].strip()\n",
    "        #     formatted_location = f\"{location_without_pincode}\\n{pincode}, {location.split(pincode)[1].strip()}\"  # Keep remaining address\n",
    "        # else:\n",
    "        #     formatted_location = location  # Fallback if pincode is not found\n",
    "    \n",
    "        # Output event details\n",
    "        f.write(f\"Event: {event_name}\\nDate: {full_date}\\nLocation: \\n{location}\\nLink: {link}\\n\\n\")\n",
    "        \n",
    "for m in months:\n",
    "    get_data_from_url(url+m, count_events, f)\n",
    "\n",
    "# get_data_from_url(url+\"january\", count_events, f)\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "07bb10b2-cfcf-433f-8351-344095b4485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to scrape\n",
    "\n",
    "import re\n",
    "\n",
    "url = \"https://downtownpittsburgh.com/events/?\"\n",
    "\n",
    "f = open(\"Events2.txt\", \"w+\")\n",
    "\n",
    "# months = [\"january\", \"february\", \"march\", \"april\", \"may\", \"june\", \"july\", \"august\", \"september\", \"october\", \"november\", \"december\"]\n",
    "# count_events = 0\n",
    "\n",
    "# Request the webpage content\n",
    "def get_data_from_url(url, file = f):\n",
    "    \n",
    "    response = requests.get(url, verify = False)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    events = soup.find_all('div', class_='copyContent')\n",
    "    \n",
    "    for event in events:\n",
    "        category = event.find('div', class_='term').get_text(strip=True) if event.find('div', class_='term') else \"No category\"\n",
    "        event_name = event.find('h1').get_text(strip=True)\n",
    "        event_date = event.find('div', class_='eventdate').get_text(strip=True)\n",
    "        # event_description = event.get_text(strip=True).split(event_name)[-1].strip()  # Get description\n",
    "        raw_description = event.get_text(strip=True).split(event_name)[-1].strip()\n",
    "        cleaned_description = re.sub(r'\\|.*?READ MORE', '', raw_description).strip()\n",
    "    \n",
    "        read_more_link = event.find('a', class_='button green right')['href']\n",
    "    \n",
    "        # Output event details\n",
    "        f.write(f\"Category: {category}\\nEvent: {event_name}\\nTime: {event_date}\\nDescription: {cleaned_description}\\nLink: {read_more_link}\\n\\n\\n\")\n",
    "        \n",
    "# for j in range(10, 13):\n",
    "#     for i in range(1,32):\n",
    "#         date = \"n=\" + str(i) + \"&d=\" + str(j) + \"&y=2024\"\n",
    "#         get_data_from_url(url + date)\n",
    "\n",
    "# for j in range(1, 10):\n",
    "#     for i in range(1,32):\n",
    "#         date = \"n=\" + str(i) + \"&d=\" + str(j) + \"&y=2025\"\n",
    "#         get_data_from_url(url + date)\n",
    "\n",
    "for i in range(10,13):\n",
    "    date = \"n=\" + str(i) + \"&y=2024&cat=0\"\n",
    "    get_data_from_url(url + date)\n",
    "for i in range(1,10):\n",
    "    date = \"n=\" + str(i) + \"&y=2025&cat=0\"\n",
    "    get_data_from_url(url + date)\n",
    "\n",
    "# get_data_from_url(url+\"january\", count_events, f)\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "d67eeaea-7249-477c-98aa-a71ef38fb343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.nhl.com/penguins/schedule/2024/fullseason <Response [200]>\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# URL to scrape\n",
    "\n",
    "import re\n",
    "\n",
    "# url = \"https://www.pghcitypaper.com/pittsburgh/EventSearch?narrowByDate=2025-01-16&sortType=date&v=d\"\n",
    "\n",
    "f = open(\"penguins_schedule.txt\", \"w+\")\n",
    "\n",
    "# Request the webpage content\n",
    "def get_data_from_url(url, file = f):\n",
    "\n",
    "    headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.5735.199 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    # Send a request with custom headers\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    # Parse the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # response = requests.get(url, verify = False)\n",
    "    # soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    print(url, response)\n",
    "    \n",
    "    match_row = soup.find_all('div', class_='sc-iqziPC')\n",
    "    print(match_row)\n",
    "\n",
    "    for math in match_row:\n",
    "        \n",
    "        team_1 = match_row.find_all('span', class_='sc-kKYJVD')[0].get_text(strip=True)\n",
    "        team_2 = match_row.find_all('span', class_='sc-kKYJVD')[1].get_text(strip=True)\n",
    "        \n",
    "        time = match_row.find('td', class_='sc-kUNLVD dvEkEQ').get_text(strip=True)\n",
    "        \n",
    "        # event_name = event.find('p', class_='fdn-teaser-headline').find('a').get_text(strip=True) if event.find('p', class_='fdn-teaser-headline') else 'N/A'\n",
    "        # event_link = event.find('p', class_='fdn-teaser-headline').find('a')['href'] if event.find('p', class_='fdn-teaser-headline') and event.find('p', class_='fdn-teaser-headline').find('a') else 'N/A'\n",
    "        # event_time = event.find('p', class_='fdn-teaser-subheadline').get_text(strip=True) if event.find('p', class_='fdn-teaser-subheadline') else 'N/A'\n",
    "        # event_venue = event.find('div', class_='uk-margin-small').find('a').get_text(strip=True) if event.find('div', class_='uk-margin-small') and event.find('div', class_='uk-margin-small').find('a') else 'N/A'\n",
    "        # event_location = event.find('p', class_='fdn-inline-split-list').get_text(strip=True) if event.find('p', class_='fdn-inline-split-list') else ''\n",
    "        # event_location += event.find('span', class_='uk-text-muted').get_text(strip=True) if event.find('span', class_='uk-text-muted') else 'N/A'\n",
    "        # event_description = event.find('div', class_='fdn-teaser-description').get_text(strip=True) if event.find('div', class_='fdn-teaser-description') else 'N/A'\n",
    "        # event_date = str(month) + \"/\" + str(date) + \"/\" + str(year)\n",
    "        # # Output event details\n",
    "        print(f\"Event: {team1}, {team2}\\nTime: {time}\\n\\n\")\n",
    "        # print(f\"Event: {event_name}, {event_date} \")\n",
    "\n",
    "        # print(f\"Price: {price_info}\\n\")me}\\nTime: {event_date}\\nDescription: {cleaned_description}\\nLink: {read_more_link}\\n\\n\\n\")\n",
    "        \n",
    "# for i in range(12,13):\n",
    "#     for j in range(29,32):\n",
    "#         if j//10 == 0:\n",
    "#             date = str(i) + \"-0\" + str(j)\n",
    "#         else:\n",
    "#             date = str(i) + \"-\" + str(j)\n",
    "#         for page in range(1, 10):\n",
    "#             if page == 1:\n",
    "#                 url = \"https://www.pghcitypaper.com/pittsburgh/EventSearch?narrowByDate=2024-\" + date + \"&sortType=date&v=d\"\n",
    "#                 get_data_from_url(url, i, j, 2024)\n",
    "#             else:\n",
    "#                 url = \"https://www.pghcitypaper.com/pittsburgh/EventSearch?narrowByDate=2024-\" + date + \"&page=\" + str(page) + \"&sortType=date&v=d\"\n",
    "#                 get_data_from_url(url, i, j, 2024)\n",
    "\n",
    "# for i in range(1,10):\n",
    "#     for j in range(1,32):\n",
    "#         if j//10 == 0:\n",
    "#             date = \"0\" + str(i) + \"-0\" + str(j)\n",
    "#         else:\n",
    "#             date = \"0\" + str(i) + \"-\" + str(j)\n",
    "#         for page in range(1, 10):\n",
    "#             if page == 1:\n",
    "#                 url = \"https://www.pghcitypaper.com/pittsburgh/EventSearch?narrowByDate=2025-\" + date + \"&sortType=date&v=d\"\n",
    "#                 get_data_from_url(url, i, j, 2025)\n",
    "#             else:\n",
    "#                 url = \"https://www.pghcitypaper.com/pittsburgh/EventSearch?narrowByDate=2025-\" + date + \"&page=\" + str(page) + \"&sortType=date&v=d\"\n",
    "#                 get_data_from_url(url, i, j, 2025)\n",
    "# driver.quit()\n",
    "\n",
    "get_data_from_url(\"https://www.nhl.com/penguins/schedule/2024/fullseason\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "5056210c-f953-4386-915b-8e9474ab4422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event: Here to Stay: Celebrating 40 Years at the Architecture Archives\n",
      "Link: /event/7307-here-to-stay-celebrating-40-years-at-the\n",
      "Time: 1/20/2025, N/A - N/A\n",
      "Location: Hunt Library, Fourth Floor\n",
      "Description: This University Libraries’ exhibition celebrates the 40th anniversary of the Architecture Archives, which was founded to create, conserve and promote the rich history of Pittsburgh’s buildings and tell part of the story of this dynamic city.\n",
      "\n",
      "\n",
      "Event: Martin Luther King Day; No Classes & University Closed\n",
      "Link: /event/12395-martin-luther-king-day-no-classes-university\n",
      "Time: 1/20/2025, N/A - N/A\n",
      "Location: \n",
      "Description: N/A\n",
      "\n",
      "\n",
      "Event: (Women’s Basketball) Brandeis vs. Carnegie Mellon\n",
      "Link: /event/11883-womens-basketball-brandeis-vs-carnegie-mellon\n",
      "Time: 1/26/2025, Noon - 3 p.m.\n",
      "Location: \n",
      "Description: N/A\n",
      "\n",
      "\n",
      "Event: (Men’s Basketball) Brandeis vs. Carnegie Mellon\n",
      "Link: /event/11882-mens-basketball-brandeis-vs-carnegie-mellon\n",
      "Time: 1/26/2025, 2 - 5 p.m.\n",
      "Location: \n",
      "Description: N/A\n",
      "\n",
      "\n",
      "Event: Here to Stay: Celebrating 40 Years at the Architecture Archives\n",
      "Link: /event/7307-here-to-stay-celebrating-40-years-at-the\n",
      "Time: 1/26/2025, N/A - N/A\n",
      "Location: Hunt Library, Fourth Floor\n",
      "Description: This University Libraries’ exhibition celebrates the 40th anniversary of the Architecture Archives, which was founded to create, conserve and promote the rich history of Pittsburgh’s buildings and tell part of the story of this dynamic city.\n",
      "\n",
      "\n",
      "Event: Here to Stay: Celebrating 40 Years at the Architecture Archives\n",
      "Link: /event/7307-here-to-stay-celebrating-40-years-at-the\n",
      "Time: 1/28/2025, N/A - N/A\n",
      "Location: Hunt Library, Fourth Floor\n",
      "Description: This University Libraries’ exhibition celebrates the 40th anniversary of the Architecture Archives, which was founded to create, conserve and promote the rich history of Pittsburgh’s buildings and tell part of the story of this dynamic city.\n",
      "\n",
      "\n",
      "Event: Here to Stay: Celebrating 40 Years at the Architecture Archives\n",
      "Link: /event/7307-here-to-stay-celebrating-40-years-at-the\n",
      "Time: 1/29/2025, N/A - N/A\n",
      "Location: Hunt Library, Fourth Floor\n",
      "Description: This University Libraries’ exhibition celebrates the 40th anniversary of the Architecture Archives, which was founded to create, conserve and promote the rich history of Pittsburgh’s buildings and tell part of the story of this dynamic city.\n",
      "\n",
      "\n",
      "Event: Carnegie Mellon Philharmonic | Student Composers Concert\n",
      "Link: https://events.time.ly/vdibqnd/43900773\n",
      "Time: 2/27/2025, 8 - 10 p.m.\n",
      "Location: Carnegie Music Hall, Oakland @ 4400 Forbes Avenue,...\n",
      "Description: N/A\n",
      "\n",
      "\n",
      "Event: Here to Stay: Celebrating 40 Years at the Architecture Archives\n",
      "Link: /event/7307-here-to-stay-celebrating-40-years-at-the\n",
      "Time: 2/27/2025, N/A - N/A\n",
      "Location: Hunt Library, Fourth Floor\n",
      "Description: This University Libraries’ exhibition celebrates the 40th anniversary of the Architecture Archives, which was founded to create, conserve and promote the rich history of Pittsburgh’s buildings and tell part of the story of this dynamic city.\n",
      "\n",
      "\n",
      "Event: Here to Stay: Celebrating 40 Years at the Architecture Archives\n",
      "Link: /event/7307-here-to-stay-celebrating-40-years-at-the\n",
      "Time: 4/24/2025, N/A - N/A\n",
      "Location: Hunt Library, Fourth Floor\n",
      "Description: This University Libraries’ exhibition celebrates the 40th anniversary of the Architecture Archives, which was founded to create, conserve and promote the rich history of Pittsburgh’s buildings and tell part of the story of this dynamic city.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# URL to scrape\n",
    "\n",
    "import re\n",
    "\n",
    "# url = \"https://events.cmu.edu/day/date/20241006\"\n",
    "\n",
    "f = open(\"Events4.txt\", \"w+\")\n",
    "\n",
    "# Request the webpage content\n",
    "def get_data_from_url(url, month, date, year, file = f):\n",
    "    \n",
    "    # response = requests.get(url, verify = False)\n",
    "    # soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    # Get the page source after interactions\n",
    "    page_source = driver.page_source\n",
    "    # Parse the page source with Beautiful Soup\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "    events = soup.find_all('div', class_='lw_cal_event')\n",
    "  \n",
    "    # print(events)\n",
    "\n",
    "    for event in events:\n",
    "        event_title = event.find('div', class_='lw_events_title').find('a').get_text(strip=True) if event.find('div', class_='lw_events_title') else \"N/A\"\n",
    "\n",
    "        # Event link\n",
    "        event_link = event.find('div', class_='lw_events_title').find('a')['href'] if event.find('div', class_='lw_events_title') else \"N/A\"\n",
    "\n",
    "        # Event time\n",
    "        start_time = event.find('span', class_='lw_start_time').get_text(strip=True) if event.find('span', class_='lw_start_time') else \"N/A\"\n",
    "        end_time = event.find('span', class_='lw_end_time').get_text(strip=True) if event.find('span', class_='lw_end_time') else \"N/A\"\n",
    "        event_time = f\"{start_time} - {end_time}\"\n",
    "        event_date = str(month) + \"/\" + str(date) + \"/\" + str(year)\n",
    "\n",
    "        # Event venue\n",
    "        event_venue = event.find('div', class_='lw_events_location').get_text(strip=True) if event.find('div', class_='lw_events_location') else \"N/A\"\n",
    "\n",
    "        # Event description\n",
    "        event_description = event.find('div', class_='lw_events_summary').get_text(strip=True) if event.find('div', class_='lw_events_summary') else \"N/A\"\n",
    "\n",
    "        # Image link (optional if present)\n",
    "        # image = event.find('img')['src'] if event.find('img') else 'No Image'\n",
    "\n",
    "        # Print or store the event details\n",
    "        # print({\n",
    "        #     'Title': event_title,\n",
    "        #     'Link': event_link,\n",
    "        #     'Time': event_time,\n",
    "        #     'Venue': event_venue,\n",
    "        #     'Description': event_description\n",
    "        #     # 'Image': image\n",
    "        # })\n",
    "        f.write(f\"Event: {event_title}\\nLink: {event_link}\\nTime: {event_date}, {event_time}\\nLocation: {event_venue}\\nDescription: {event_description}\\n\\n\")\n",
    "        print(f\"Event: {event_title}\\nLink: {event_link}\\nTime: {event_date}, {event_time}\\nLocation: {event_venue}\\nDescription: {event_description}\\n\\n\")\n",
    "\n",
    "# get_data_from_url(\"https://events.cmu.edu/\", 10, 6, 2024)\n",
    "# for i in range(10,13):\n",
    "    # for j in range(1,10):\n",
    "    #     date = str(i) + \"0\" + str(j)\n",
    "    #     url = \"https://events.cmu.edu/day/date/2024\" + date\n",
    "    #     get_data_from_url(url, i, j, 2024)\n",
    "    # for j in range(20,32):\n",
    "    #     date = str(i) + str(j)\n",
    "    #     url = \"https://events.cmu.edu/day/date/2024\" + date\n",
    "    #     get_data_from_url(url, i, j, 2024)\n",
    "\n",
    "# for i in range(1,10):\n",
    "    # for j in range(1,10):\n",
    "    #     date = \"0\" + str(i) + \"0\" + str(j)\n",
    "    #     url = \"https://events.cmu.edu/day/date/2025\" + date\n",
    "    #     get_data_from_url(url, i, j, 2025)\n",
    "    # for j in range(20,32):\n",
    "    #     date = \"0\" + str(i) + str(j)\n",
    "    #     url = \"https://events.cmu.edu/day/date/2025\" + date\n",
    "    #     get_data_from_url(url, i, j, 2025)\n",
    "        \n",
    "\n",
    "# for i in range(1,10):\n",
    "#     for j in range(1,32):\n",
    "#         date = \"0\" + str(i) + \"-\" + str(j)\n",
    "#         for page in range(1, 10):\n",
    "#             if page == 1:\n",
    "#                 url = \"https://www.pghcitypaper.com/pittsburgh/EventSearch?narrowByDate=2025-\" + date + \"&sortType=date&v=d\"\n",
    "#                 get_data_from_url(url, i, j, 2025)\n",
    "#             else:\n",
    "#                 url = \"https://www.pghcitypaper.com/pittsburgh/EventSearch?narrowByDate=2025-\" + date + \"&page=\" + str(page) + \"&sortType=date&v=d\"\n",
    "#                 get_data_from_url(url, i, j, 2025)\n",
    "# get_data_from_url(url+\"january\", count_events, f)\n",
    "# driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "dda8ecc7-9a38-42f7-9fdd-5776d051bb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('penguinsschedule.csv')\n",
    "f = open(\"penguins_schedule.txt\", \"w+\")\n",
    "\n",
    "# Display the entire CSV\n",
    "# print(df)\n",
    "\n",
    "# To access specific event details, you can loop through the DataFrame rows:\n",
    "for index, row in df.iterrows():\n",
    "    f.write(f\"Match: {row['Event Name']}\\n\")\n",
    "    f.write(f\"Date: {row['Date']}\\n\")\n",
    "    f.write(f\"Time: {row['Time']}\\n\")\n",
    "    f.write(f\"Location: {row['Location']}\\n\")\n",
    "    f.write(\"\\n\\n\")\n",
    "    # print(f\"Match: {row['Event Name']}\\n\")\n",
    "    # print(f\"Date: {row['Date']}\\n\")\n",
    "    # print(f\"Time: {row['Time']}\\n\")\n",
    "    # print(f\"Location: {row['Location']}\\n\")\n",
    "    # print(\"\\n\\n\")\n",
    "    \n",
    "#     print(f\"Location: {row['LOCATION']}\")\n",
    "#     print(f\"Description: {row['DESCRIPTION']}\")\n",
    "#     print(f\"End Date: {row['END DATE']}\")\n",
    "#     print('-' * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51b3aa98-c0fd-4c54-acd2-655db07bd323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All text files have been combined into ./text scraping/all_sports.txt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Specify the directory containing the text files and the output file path\n",
    "input_directory = \"./text scraping/sports/\"\n",
    "output_file = \"./text scraping/all_sports.txt\"\n",
    "\n",
    "# Open the output file in write mode\n",
    "with open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "    # Iterate through all files in the directory\n",
    "    for filename in os.listdir(input_directory):\n",
    "        # Check if the file is a .txt file\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(input_directory, filename)\n",
    "            # Open and read the content of each file\n",
    "            with open(file_path, 'r') as infile:\n",
    "                content = infile.read()\n",
    "                # Write the content to the output file\n",
    "                outfile.write(content + \"\\n\")  # Add a newline between file contents if needed\n",
    "\n",
    "print(f\"All text files have been combined into {output_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bccf45a-523c-43ef-a0dd-69b6c5e1c05f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
