{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tdyy343_5UPe",
    "outputId": "a8d1fb8d-1d4e-4d3f-9260-ce12c6ef3fc3"
   },
   "outputs": [],
   "source": [
    "!pip install faiss-cpu\n",
    "\n",
    "# Install Ollama Python SDK\n",
    "!pip install ollama\n",
    "\n",
    "# Install Numpy\n",
    "!pip install numpy\n",
    "\n",
    "# (Optional) Install Transformers if you need it for other models\n",
    "!pip install sentence_transformers\n",
    "\n",
    "# (Optional) Install Pandas if you need it for data handling\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zY6iCGvn5BPM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMU_Traditions.txt\n",
      "CMU_Wikipedia.txt\n",
      "Transportation_Pitts_Wikipedia.txt\n",
      "Pittsburgh_Wikipedia.txt\n",
      "festivals.txt\n",
      "Visit_Pittsburgh.txt\n",
      "current_affairs.txt\n",
      "City_Of_Pittsbugh.txt\n",
      "LocalService_Regulations.txt\n",
      "opera_events.txt\n",
      "ISP_Regulations.txt\n",
      "PIRATES.txt\n",
      "pgh_cultural_trust.txt\n",
      "PENGUINS.txt\n",
      "CMU_History.txt\n",
      "UF_Regulations.txt\n",
      "carnegie_museums.txt\n",
      "Bridges_Wikipedia.txt\n",
      "STEELERS.txt\n",
      "Events_Pittsburgh.txt\n",
      "sports_teams.txt\n",
      "wikis.txt\n",
      "Events_CMU.txt\n",
      "annual_fests.txt\n",
      "CMU_Events_Extra.txt\n",
      "sports_schedules.txt\n",
      "History_WIkipedia.txt\n",
      "museums_list.txt\n",
      "CMU.txt\n",
      "Visit_Pitts 1.txt\n",
      "symphony.txt\n",
      "Culture_Pitts_wikipedia.txt\n",
      "Parking_Regulations.txt\n",
      "Amusement_Regulations.txt\n",
      "CMU_Welcome_To_Pittsburgh.txt\n",
      "Payroll_Regulations.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory containing text files\n",
    "directory = \"../data/\"\n",
    "\n",
    "# Read all text files and combine them\n",
    "all_texts = []\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        print(filename)\n",
    "        with open(os.path.join(directory, filename), 'r', encoding='utf-8') as file:\n",
    "            all_texts.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "HkFD6uTt5MOm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2909\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text, max_length=128):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "\n",
    "    for i in range(0, len(words), max_length):\n",
    "        chunks.append(\" \".join(words[i:i + max_length]))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Create chunks from each file content\n",
    "text_chunks = []\n",
    "for text in all_texts:\n",
    "    text_chunks.extend(chunk_text(text, max_length=256))\n",
    "print(len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_DwTBSf96DU1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load a pre-trained model for embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings = model.encode(text_chunks, convert_to_tensor=True)\n",
    "embedding_dim = doc_embeddings.shape[1]\n",
    "\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(doc_embeddings.cpu().numpy())\n",
    "\n",
    "faiss.write_index(index, 'faiss_index.index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "lTJ-7Smp6527"
   },
   "outputs": [],
   "source": [
    "def retrieve_documents(query, k=3):\n",
    "    query_embedding = model.encode([query])\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    matching_chunks = [text_chunks[i] for i in indices[0]]\n",
    "    return matching_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {
    "id": "hazsPZpy5q64"
   },
   "outputs": [],
   "source": [
    "def generate_qa_pairs(context, num_questions=1):\n",
    "    # Define the prompt to instruct the model to generate questions based on the context\n",
    "    prompt = f\"\"\"\n",
    "    Read the following context and generate {num_questions} possible question-answer pairs:\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Provide the output in the following format:\n",
    "    Q1: <Question>\n",
    "    A1: <Answer1>\n",
    "    Q2: <Question>\n",
    "    A2: <Answer2>\n",
    "    ...\n",
    "    \n",
    "    Here are examples of the types of questions and answers you should generate:\n",
    "    \n",
    "    1. Give direct and concise answers, preferably in least words.\n",
    "    For example:\n",
    "    Q1: What are the operating hours of the CMU Pantry?\n",
    "    A1: 2 p.m. to 5 p.m.\n",
    "    \n",
    "    3. Ensure that each question can be answered from the context and is not vague or ambiguous.\n",
    "    4. Generate multiple unrelated questions for similar topics through paraphrasing and identifying key information.\n",
    "    6. Reduce questions for which no context is available.\n",
    "    7. Each question should be answerable independently, do not return questions dependent on any previous answer.\n",
    "    For example:\n",
    "    Q3: When is the X event happening?\n",
    "    should not be followed by:\n",
    "    Q4. What is it's location?\n",
    "    Instead, it should be followed by:\n",
    "    Q4: What is the location of the X event?\n",
    "\n",
    "    8. While answering a question about some event X, do not use \"it\" or \"the event\".\n",
    "    For example:\n",
    "    Q3: How long does the Gender in Process event last?\n",
    "    Wrong answer: This event runs from 3:30 to 5 p.m.\n",
    "    Correct answer: 3:30 to 5 p.m.\n",
    "\n",
    "    Phrase the answer well - don't give answers in points like \"Week 12, · Sun 11/21, · 8:15 PM EST.\", instead the answer should be \"Sunday, November 21st, 2024, 8:15 PM EST\"\n",
    "    \"\"\"\n",
    "\n",
    "    # Send the prompt to the Ollama server\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"llama3.2\",\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,  # Set to True if you want the response streamed back\n",
    "        # \"top_p\": 0.9\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(url, json=payload)\n",
    "        response.raise_for_status()  # Raise an error for bad responses\n",
    "        result = response.json().get('response', \"\")\n",
    "        \n",
    "        # Parse the output to separate questions and answers\n",
    "        questions, answers = [], []\n",
    "        lines = result.strip().split(\"\\n\")\n",
    "        \n",
    "        for line in lines:\n",
    "            if line.startswith(\"Q\"):\n",
    "                questions.append(line.split(\": \", 1)[1].strip())\n",
    "            elif line.startswith(\"A\"):\n",
    "                answers.append(line.split(\": \", 1)[1].strip())\n",
    "        \n",
    "        return questions, answers\n",
    "\n",
    "    except requests.ConnectionError:\n",
    "        print(\"Connection error: Could not connect to the Ollama server.\")\n",
    "        return [], []\n",
    "    except requests.HTTPError as http_err:\n",
    "        print(f\"HTTP error occurred: {http_err}\")\n",
    "        return [], []\n",
    "    except Exception as err:\n",
    "        print(f\"An error occurred: {err}\")\n",
    "        return [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "qa_df = pd.DataFrame(columns=[\"question\", \"answer\", \"context\"])\n",
    "\n",
    "questions_list = []\n",
    "answers_list = []\n",
    "count_chunks = 641\n",
    "\n",
    "# for chunk in text_chunks:\n",
    "#     # print(chunk)\n",
    "#     fq = open(\"questions\" + str(count_chunks) + \".txt\", \"w+\")\n",
    "#     fa = open(\"answers\" + str(count_chunks) + \".txt\", \"w+\")\n",
    "#     questions, answers = generate_qa_pairs(chunk)\n",
    "#     for question in questions:\n",
    "#         fq.write(question + \"\\n\")\n",
    "#     for answer in answers:\n",
    "#         fa.write(answer + \"\\n\")\n",
    "#     # questions_list.extend(questions)\n",
    "#     # answers_list.extend(answers)\n",
    "#     count_chunks+=1\n",
    "\n",
    "for chunk in text_chunks:\n",
    "    questions, answers = generate_qa_pairs(chunk)\n",
    "    print(questions,answers)\n",
    "    # for question in questions:\n",
    "    #     retrieved_chunks = retrieve_documents(question)\n",
    "    #     context = \" \".join(retrieved_chunks)\n",
    "    #     answer = ollama_generate(question, context)\n",
    "    #     new_row = pd.DataFrame({\"question\": [question], \"answer\": [answer], \"context\": [context]})\n",
    "    #     qa_df = pd.concat([qa_df, new_row], ignore_index=True)\n",
    "\n",
    "# for i in range(len(questions_list)):\n",
    "#     print(f\"Question: {questions_list[i]}\\nAnswer: {answers_list[i]}\\n\")\n",
    "# print(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_df.to_csv(\"qa_pairs_sports_schedule.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
